{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Luke Hayes - Deep Learning Assignment 1 - 14498098"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "# Display plots inline and change default figure size\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1 - Create Logistic Regression Algorithm** \n",
    "\n",
    "The get_data function below is used to take in csv file with the data and parse it into the format that we require. It does the splitting of the train, test and validation data and also splits up the attributes and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df):\n",
    "    #HERE WE SPLIT THE TRAINING DATA \n",
    "    train, test = train_test_split(df, test_size=0.3)\n",
    "    val, test = train_test_split(test, test_size=0.5)\n",
    "\n",
    "    #ISOLATE JUST THE VALUES AND NOT THE COLUMN HEADERS\n",
    "    train_set = train.values\n",
    "    test_set = test.values\n",
    "    val_set = val.values \n",
    "\n",
    "    #ISOLATE THE Y VALUES AND DELETE Y VALUES SO WE CAN ISOLATE ATTRIBUTES\n",
    "    Ytrain = train['Class'].values\n",
    "    Ytest = test['Class'].values\n",
    "    Yval = val['Class'].values\n",
    "\n",
    "    del train['Class']   \n",
    "    del test['Class']   \n",
    "    del val['Class']   \n",
    "\n",
    "    #GET THE VALUES OF THE X VALUES\n",
    "    Xtrain = train.values     \n",
    "    Xtest = test.values     \n",
    "    Xval = val.values \n",
    "    \n",
    "    x, num_attr = Xval.shape\n",
    "\n",
    "    return num_attr, Xtrain, Xtest, Xval, Ytrain, Ytest, Yval, train_set, test_set, val_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next number of functions are used to run logistic regression on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regressor(num_atr, max_iter, train, alpha, threshold):\n",
    "    \n",
    "    #INITIALISE THE WEIGHTS TO BE SMALL VALUES CLOSE TO 0\n",
    "    w = np.random.normal(0,0.01,size=(num_atr))\n",
    "    b = np.random.normal(0,0.01,size=(1))\n",
    "    j_cur = 0\n",
    "    \n",
    "    delta_w = np.random.normal(0,0,size=(num_atr))\n",
    "\n",
    "    #LOOP FOR MAX ITERATIONS\n",
    "    for i in range(max_iter):\n",
    "        \n",
    "        #GET A RANDOME SAMPLE\n",
    "        res = random.sample(list(train), 1)[0]\n",
    "        #GET THE Y VALUE FOR THAT SAMPLE\n",
    "        yval = res[-1]\n",
    "        #GET THE X VALUES OF THE SAMPLE\n",
    "        x = np.delete(res,-1,0)\n",
    "        \n",
    "        #GET Y HAT - THE PREDICTED VALUE \n",
    "        y_hat = get_yhat(w,x,b)\n",
    "        \n",
    "        #PREVIOUS VALUE IS EQUAL TO THE LAST ITERATION VALUE\n",
    "        j_prev = j_cur\n",
    "        #GET THE COST FUNCTION VALUE\n",
    "        j_cur = -(yval*np.log(y_hat)+(1-yval)*np.log(1-y_hat))\n",
    "        \n",
    "        #IF THE CHANGE IN THE COST FUNCTION VALUE IS LESS THAN THE THRESHOLD FINISH THE FUNCTION\n",
    "        if(i!=0 and abs(j_prev - j_cur) <= threshold):\n",
    "            w = np.append(w,b)\n",
    "            return w\n",
    "        \n",
    "        #GET DELTA W AND DELTA B\n",
    "        for j in range(len(w)):\n",
    "            delta_w[j] = (y_hat - yval)* x[j] \n",
    "        delta_b = y_hat - yval\n",
    "        \n",
    "        #UPDATE THE WEIGHTS AND BIAS USING DELTA W AND DELTA B AND THE LEARNING RATE\n",
    "        for k in range(len(w)):\n",
    "            w[k] -= alpha * delta_w[k]\n",
    "            #print(w)\n",
    "        b -= alpha * delta_b\n",
    "        \n",
    "    #RETURN THE WEIGHTS\n",
    "    return w\n",
    "\n",
    "def get_yhat(w,x,b):\n",
    "    #GET THE DOT PRODUCT OF THE WEIGHTS AND THE X VALUES AND ADD THE BIAS\n",
    "    val = np.dot(w,x)\n",
    "    z = val + b\n",
    "    #GET THE SIGMOID VALUE OF THE OUTPUT\n",
    "    y = 1/(1+np.exp(-z))\n",
    "    return y\n",
    "\n",
    "\n",
    "def runLogisticRegression(num_attr, max_iter, train, test, learn_rate, thresh):\n",
    "\n",
    "    x = logistic_regressor(num_attr, 10000000, train, 0.001, 0.000001)\n",
    "\n",
    "    #GET THE TRAINING VALUES AND THE BIAS FROM THE INPUT DATA\n",
    "    train_bias = x[-1]\n",
    "    train_weights = np.delete(x,-1,0)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    #LOOP THROUGH ALL TEST SAMPLES\n",
    "    for t in test:\n",
    "        \n",
    "        #GET THE Y VALUE AND X VALUES \n",
    "        test_yval = t[-1]\n",
    "        x = np.delete(t,-1,0)\n",
    "        \n",
    "        #MULTIPLY THE X VALUES BY THE TRAINING WEIGHTS \n",
    "        test_val = np.dot(train_weights,x)\n",
    "        #ADD THE BIAS\n",
    "        testz = test_val + train_bias\n",
    "        #GET THE SIGMOID VALUE\n",
    "        test_y_hat = 1/(1+np.exp(-testz))\n",
    "\n",
    "        #print(\"This is the prediected value %f and this is the actual value %d\" %(test_y_hat ,test_yval))\n",
    "        if (test_y_hat >= 0.5):\n",
    "            test_y_hat = 1\n",
    "        else:\n",
    "            test_y_hat = 0\n",
    "        if (test_y_hat == test_yval):\n",
    "            correct = correct + 1\n",
    "            total = total + 1\n",
    "        else:\n",
    "            total = total + 1\n",
    "\n",
    "    accuracy = (correct/total)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 10000000\n",
    "learn_rate = 0.001\n",
    "thresh = 0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2 - Test Logistic Regression Algorithm**\n",
    "\n",
    "Here we then run logistic regression on the linearly seperable and non-linearly seperable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the linearly seperable dataset:  1.0\n",
      "\n",
      "\n",
      "Accuracy for the non-linearly seperable dataset:  0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:/Users/lhaye/Downloads/blobs250.csv\")\n",
    "\n",
    "num_attr, Xtrain, Xtest, Xval, Ytrain, Ytest, Yval, train_set, test_set, val_set = get_data(df)\n",
    "accuracy = runLogisticRegression(num_attr, max_iter, train_set, test_set, learn_rate, thresh)\n",
    "\n",
    "print(\"Accuracy for the linearly seperable dataset: \", accuracy)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/lhaye/Downloads/moons400.csv\")\n",
    "\n",
    "num_attr, Xtrain, Xtest, Xval, Ytrain, Ytest, Yval, train_set, test_set, val_set = get_data(df)\n",
    "accuracy = runLogisticRegression(num_attr, max_iter, train_set, test_set, learn_rate, thresh)\n",
    "\n",
    "print(\"Accuracy for the non-linearly seperable dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the Logistic regressor performs with an accuracy of 100% on the test set of the linearly seperable data. The algorithm surprisingly achieves a very high accuracy of 83% on the non-linearly seperable dataset. This is an excellent result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3 - Shallow Neural Network**\n",
    "\n",
    "Next we have the implementation of the shallow neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1033,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "USE NUMPY VECTORIZATION AS FAR AS POSSIBLE TO SPEED UP BOTH TRAINING AND TESTING\n",
    "\"\"\"\n",
    "#SIGMOID FUNCTION\n",
    "def sig(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "#SIGMOID DERIVATIVE FUNCTION \n",
    "def sig_deriv(x):\n",
    "    return np.multiply(sig(x), 1 - sig(x))\n",
    "\n",
    "#NEURAL NETWORK TRAINING\n",
    "def training(num_attr, max_iter, attribute, label, learn_rate, thresh):\n",
    "    \n",
    "    #INITIALIZATION     \n",
    "    hid_size = 600\n",
    "    input_size = num_attr\n",
    "    out_size = 1\n",
    "    \n",
    "    #INITIALISE WEIGHTS AND BIAS\n",
    "    hid_w = np.random.uniform(-1,1,size=(input_size , hid_size))\n",
    "    out_w = np.random.uniform(-1,1,size=(hid_size))\n",
    "    b1 = np.random.uniform(-0.5,0.5,size=(hid_size))\n",
    "    b2 = np.random.uniform(-0.5,0.5,size=(out_size))\n",
    "    \n",
    "    j_cur = 0\n",
    "    epoch = 0\n",
    "    cost_total = 0\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        \n",
    "        #GET INDEX OF RANDOM TRAINING SAMPLE\n",
    "        select = random.randint(0, len(attribute) - 1)\n",
    "\n",
    "        #GET THE TRAINING VALUES AND THE OUTPUT(Y) VALUE\n",
    "        x = attribute[select]\n",
    "        yval = label[select]\n",
    "\n",
    "        ######################\n",
    "        #FORWARD PROPOGATION\n",
    "        ######################\n",
    "    \n",
    "    \n",
    "        hid_node_z = np.dot(x,hid_w)\n",
    "        hid_node_z = np.add(hid_node_z, b1)\n",
    "        hid_node = sig(hid_node_z)\n",
    "                        \n",
    "        output = np.dot(hid_node, out_w) + b2\n",
    "        \n",
    "        #GET OUTPUT\n",
    "        y_hat = sig(output)\n",
    "                \n",
    "        j_prev = j_cur\n",
    "        \n",
    "        #GET COST \n",
    "        j_cur = -(yval*np.log(y_hat)+(1-yval)*np.log(1-y_hat))\n",
    "        \n",
    "        #APPEND IT TO A TOTAL FOR THE EPOCH \n",
    "        cost_total += j_cur\n",
    "\n",
    "        #IF WE HIT 1675 ITERATIONS WE HAVE DONE ONE EPOCH\n",
    "        if(i%1675==0):\n",
    "\n",
    "            epoch = epoch + 1\n",
    "            print(\"Cost function for this epoch is \", (cost_total/1675))\n",
    "            cost_total = 0\n",
    "\n",
    "        #IF WE REACH MAX EPOCHS OR THE THRESHOLD WE RETURN\n",
    "        if(i!=0 and abs(j_prev - j_cur) <= thresh or epoch==40):\n",
    "            return hid_w, out_w, b1, b2, hid_size\n",
    "        \n",
    "        \n",
    "        \n",
    "        ##################\n",
    "        #BACK PROPOGATION\n",
    "        ##################\n",
    "        delta_z_out = y_hat - yval\n",
    "        #MIGHT NEED TO REMOVE THIS\n",
    "        delta_w_out = delta_z_out * hid_node \n",
    "        delta_b_out = delta_z_out\n",
    "        \n",
    "        \n",
    "        #NOW WE BACKPROPOGATE TO THE HIDDEN LAYER \n",
    "        #WE NEED DETLA Z OF THE HIDDEN LAYER FOR EACH NODE\n",
    "        deriv_activ = [sig_deriv(x) for x in hid_node_z]        \n",
    "        delta_z_hid = np.multiply(delta_z_out, out_w)    \n",
    "        delta_z_hidden = np.multiply(delta_z_hid, deriv_activ)\n",
    "        \n",
    "        #GET DELTA W FOR EACH NODE OF THE HIDDEN LAYER\n",
    "        delta_w_hidden = np.multiply(delta_z_hidden,hid_node)\n",
    "        \n",
    "        #DELTA B IS EQUAL TO DELTA Z \n",
    "        delta_b_hidden = delta_z_hidden\n",
    "        \n",
    "        #######################################################################################\n",
    "        #MODIFY THE WEIGHTS AND THE BIAS VALUES USING THE CALCUALTED VALUES AND LEARNING RATE\n",
    "        #######################################################################################\n",
    "        val = np.multiply(learn_rate,delta_b_hidden)\n",
    "        b1 = np.subtract(b1, val)\n",
    "        \n",
    "        val = np.multiply(learn_rate,delta_w_hidden)\n",
    "        hid_w = np.subtract(hid_w, val)\n",
    "        \n",
    "        val = np.multiply(learn_rate,delta_w_out)\n",
    "        out_w = np.subtract(out_w, val)\n",
    "        \n",
    "        val = np.multiply(learn_rate,delta_b_out)\n",
    "        b2 = np.subtract(b2, val)\n",
    "        \n",
    "    return hid_w, out_w, b1, b2, hid_size\n",
    "        \n",
    "\n",
    "#THIS FUNCTION TAKES IN THE TRAINING WEIGHTS AND TRIED TO PREDICT THE VALUES OF THE TEST SAMPLES\n",
    "#AN ACCURACY IS THEN RETURNED BASED ON HOW WELL THE NEURAL NETWORK PREDICTS THE VALUES\n",
    "def predict(hidden_weights, output_weights, bias_1, bias_2, attributes, labels, hid_size):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    #LOOP THROUGH ALL THE TEST CASES \n",
    "    for t in range(len(attributes)):        \n",
    "        \n",
    "        #FORWARD PROPAGATION\n",
    "        #HIDDEN LAYER\n",
    "        \n",
    "        #print(attributes[t])\n",
    "        #print(hidden_weights)\n",
    "        hid_node_z = np.dot(attributes[t],hidden_weights) + b1\n",
    "        hid_node = sig(hid_node_z)\n",
    "            \n",
    "        #OUTPUT LAYER\n",
    "        output = np.dot(hid_node, output_weights) + b2 \n",
    "        y_hat = sig(output)\n",
    "        \n",
    "        #IF THE YHAT VALUE IS ABOVE 0.5 SET IT TO 1 AND IF LESS SET TO 0\n",
    "        if y_hat >= 0.5:\n",
    "            y_hat = 1\n",
    "        else:\n",
    "            y_hat = 0\n",
    "            \n",
    "        #IF THE PREDICTION IS CORRECT INCREASE CORRECT \n",
    "        if (y_hat == labels[t]):\n",
    "            correct = correct + 1\n",
    "            total = total + 1\n",
    "        else:\n",
    "            total = total + 1\n",
    "        \n",
    "        #print(\"This is the prediected value %f and this is the actual value %d\" %(y_hat ,labels[t]))\n",
    "    \n",
    "    #RETURN ACCURACY\n",
    "    accuracy = (correct/total)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4 - Test Shallow Neural Network**\n",
    "\n",
    "Now we will run the shallow neural network on both datasets as was done with the logistic regression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost function for this epoch is  [0.00031455]\n",
      "Accuracy for the linearly seperable dataset:  1.0\n",
      " \n",
      " \n",
      "Cost function for this epoch is  [3.23859413e-06]\n",
      "Cost function for this epoch is  [0.64465785]\n",
      "Cost function for this epoch is  [0.40931941]\n",
      "Cost function for this epoch is  [0.35391735]\n",
      "Cost function for this epoch is  [0.30273805]\n",
      "Cost function for this epoch is  [0.31922518]\n",
      "Cost function for this epoch is  [0.30878181]\n",
      "Cost function for this epoch is  [0.30512534]\n",
      "Cost function for this epoch is  [0.2997023]\n",
      "Cost function for this epoch is  [0.28623136]\n",
      "Cost function for this epoch is  [0.28976325]\n",
      "Cost function for this epoch is  [0.28038651]\n",
      "Cost function for this epoch is  [0.26737223]\n",
      "Cost function for this epoch is  [0.29590647]\n",
      "Cost function for this epoch is  [0.29718394]\n",
      "Cost function for this epoch is  [0.31688854]\n",
      "Cost function for this epoch is  [0.29058712]\n",
      "Cost function for this epoch is  [0.28815536]\n",
      "Cost function for this epoch is  [0.29728738]\n",
      "Cost function for this epoch is  [0.29750566]\n",
      "Cost function for this epoch is  [0.29523669]\n",
      "Cost function for this epoch is  [0.27843122]\n",
      "Cost function for this epoch is  [0.27448692]\n",
      "Cost function for this epoch is  [0.29802238]\n",
      "Cost function for this epoch is  [0.28409517]\n",
      "Cost function for this epoch is  [0.29167622]\n",
      "Accuracy for the non-linearly seperable dataset:  0.9\n"
     ]
    }
   ],
   "source": [
    "max_iter = 100000\n",
    "learn_rate = 0.001\n",
    "thresh = 0.000001\n",
    "    \n",
    "#TEST ON LINEARLY SEPERABLE DATASET\n",
    "df = pd.read_csv(\"C:/Users/lhaye/Downloads/blobs250.csv\")\n",
    "num_attr, Xtrain, Xtest, Xval, Ytrain, Ytest, Yval, train_set, test_set, val_set = get_data(df)\n",
    "    \n",
    "hid_w, out_w, b1, b2, hid_size = training(num_attr, max_iter, Xtrain, Ytrain, learn_rate, thresh)\n",
    "\n",
    "accuracy = predict(hid_w, out_w, b1, b2, Xtest, Ytest, hid_size)\n",
    "print(\"Accuracy for the linearly seperable dataset: \", accuracy)\n",
    "\n",
    "\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "\n",
    "#TEST ON NON-LINEARLY SEPERABLE DATASET\n",
    "df = pd.read_csv(\"C:/Users/lhaye/Downloads/moons400.csv\")\n",
    "num_attr, Xtrain, Xtest, Xval, Ytrain, Ytest, Yval, train_set, test_set, val_set = get_data(df)\n",
    "    \n",
    "hid_w, out_w, b1, b2, hid_size = training(num_attr, max_iter, Xtrain, Ytrain, learn_rate, thresh)\n",
    "\n",
    "accuracy = predict(hid_w, out_w, b1, b2, Xtest, Ytest, hid_size)\n",
    "print(\"Accuracy for the non-linearly seperable dataset: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this shallow neural network performs with an accuracy of 100% on the test set of the linearly seperable data. There is also an accuracy of 90% achieved on the non-linear dataset. This is an increase of over 7% when compared to the logistic regression algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move on to getting the Cifar data and manipulating it into a way that we can feed it into the neural network. First read in the batch, then convert each image in the batch to a 1024 array. Then remove the indexes that are not an automobile or horse. Next change the values of the horse and automobile class to 0 and 1. Finally split the data into training and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOW LETS MOVE ON TO THE IMAGE DATASET TASK \n",
    "import pickle\n",
    "import io\n",
    "\n",
    "#FISRT TWO FUNCTIONS TAKE FROM LECTURE CODE\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def loadbatch(batchname):\n",
    "    folder = 'C:/Users/lhaye/Downloads/cifar-10-batches-py'\n",
    "    batch = unpickle(folder+\"/\"+batchname)\n",
    "    return batch\n",
    "\n",
    "def load_cifar_data():\n",
    "    batch1 = loadbatch('data_batch_1')\n",
    "\n",
    "    attr = batch1[b'data']\n",
    "    labels = batch1[b'labels']\n",
    "    \n",
    "    #READ IN EACH IMAGE IN AS 1024 AS REQUIRED\n",
    "    #THEREFORE THERE WILL BE 1024 INPUTS INTO THE NEURAL NETWORK\n",
    "    num_attr = 1024\n",
    "    #EACH ATTR IS AN IMAGE INPUT\n",
    "    attr  = [image[0:num_attr] for image in attr]\n",
    "\n",
    "    #HERE WE JUST TAKE THE INDEXES IF THE LABELS ARE 1 OR 7 AS THIS IS THE INDEX OF HORSE AND AUTOMOBILE\n",
    "    indexes = [index for index in range(len(labels)) if (labels[index] == 1 or labels[index] == 7)]\n",
    "    #UPDATE THE ATTRIBUTES TO JUST HAVE THE IMAGES WE WANT ONLY\n",
    "    attr = [attr[index] for index in indexes]\n",
    "    #UPDATE THE LABELS ALSO \n",
    "    labels = [labels[index] for index in indexes]\n",
    "\n",
    "    #NOW LETS SET THE LABEL VALUE OF 7 TO 0 AS AUTOMOBILE IS ALREADY 1 SO HORSE WILL NOW BE 0 \n",
    "    #THIS MAKES CLASSIFICATION EASIER LATER\n",
    "    for key, val in enumerate(labels):\n",
    "        if(val == 7):\n",
    "            labels[key] = 0\n",
    "\n",
    "    #SPLIT DATA INTO TRAIN AND TEST AND NORMALIZE\n",
    "    train_attr = np.array(attr[300:]) / 255 \n",
    "    train_labels = np.array(labels[300:]).T\n",
    "\n",
    "    test_attr = np.array(attr[:300]) / 255 \n",
    "    #print(len(train_attr))\n",
    "    test_labels = np.array(labels[:300]).T\n",
    "    \n",
    "    val = train_attr[0]\n",
    "\n",
    "    return num_attr, train_attr, test_attr, train_labels, test_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run this shallow neural network on the Cifar data that has just been prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost function for this epoch is  [0.0077466]\n",
      "Cost function for this epoch is  [1.1354225]\n",
      "Cost function for this epoch is  [0.90812979]\n",
      "Cost function for this epoch is  [0.78954652]\n",
      "Cost function for this epoch is  [0.72989067]\n",
      "Cost function for this epoch is  [0.64835109]\n",
      "Cost function for this epoch is  [0.67282463]\n",
      "Cost function for this epoch is  [0.6099486]\n",
      "Cost function for this epoch is  [0.59159007]\n",
      "Cost function for this epoch is  [0.55620834]\n",
      "Cost function for this epoch is  [0.58020773]\n",
      "Cost function for this epoch is  [0.57579076]\n",
      "Cost function for this epoch is  [0.50394152]\n",
      "Cost function for this epoch is  [0.4878809]\n",
      "Cost function for this epoch is  [0.49285904]\n",
      "Cost function for this epoch is  [0.45893646]\n",
      "Cost function for this epoch is  [0.44502811]\n",
      "Cost function for this epoch is  [0.45138609]\n",
      "Cost function for this epoch is  [0.43990271]\n",
      "Cost function for this epoch is  [0.45937504]\n",
      "Accuracy for the Neural Network on the Cifar Dataset - Horse vs Automobile Classification:  0.72\n"
     ]
    }
   ],
   "source": [
    "max_iter = 16750000\n",
    "learn_rate = 0.001\n",
    "thresh = 0.000001\n",
    "\n",
    "num_attr, train_attributes, test_attributes, train_labels, test_labels= load_cifar_data()\n",
    "\n",
    "hid_w, out_w, b1, b2, hid_size = training(num_attr, max_iter, train_attributes, train_labels, learn_rate, thresh)\n",
    "\n",
    "accuracy = predict(hid_w, out_w, b1, b2, test_attributes, test_labels, hid_size)\n",
    "print(\"Accuracy for the Neural Network on the Cifar Dataset - Horse vs Automobile Classification: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy value for the shallow neural network on the Cifar image test set is 75%. This result is quite impressive considering we have just used the image alone. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 5 - Neural Network Enhancement - Adaptable Number of Hidden Layers**\n",
    "The enhancement I chose was a neural network with an adaptable number of hidden layers. This meant that any number could be passed into the algorithm to create N number of hidden layers. For the sake of simplicity each hidden layer has the same number of nodes and this is something that could be further developed in future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "USE NUMPY VECTORIZATION AS FAR AS POSSIBLE TO SPEED UP BOTH TRAINING AND TESTING\n",
    "\"\"\"\n",
    "#SIGMOID FUNCTION\n",
    "def sig(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "#SIGMOID DERIVATIVE FUNCTION \n",
    "def sig_deriv(x):\n",
    "    return np.multiply(sig(x), 1 - sig(x))\n",
    "\n",
    "\n",
    "def training2(num_attr, max_iter, attribute, label, learn_rate, thresh, hid_layers):\n",
    "    \n",
    "    #INITIALIZATION     \n",
    "    hid_size = 600\n",
    "    input_size = num_attr\n",
    "    out_size = 1\n",
    "    \n",
    "    \n",
    "    #INITIALISE WEIGHTS AND BIAS\n",
    "    out_w = np.random.uniform(-1,1,size=(hid_size))\n",
    "    b_out = np.random.uniform(-0.5,0.5,size=(out_size))\n",
    "    \n",
    "    hidnew = {}\n",
    "    b = {}\n",
    "    j_cur = 0\n",
    "    cost_total = 0\n",
    "    epoch = 0\n",
    "    \n",
    "    \n",
    "    #HERE WE CREATE THE WEIGHTS FOR THE SELECTED NUMBER OF HIDDEN LAYERS AND PUT THE WEIGHTS INTO A DICTIONARY\n",
    "    for i in range(hid_layers):\n",
    "        if (i == 0):\n",
    "            hidnew[\"hlayer\" + str(i)] = np.random.uniform(-1,1,size=(input_size , hid_size))\n",
    "            b[\"hlayer\" + str(i)] = np.random.uniform(-0.5,0.5,size=(hid_size))\n",
    "\n",
    "        else:\n",
    "            hidnew[\"hlayer\" + str(i)] = np.random.uniform(-1,1,size=(hid_size , hid_size))\n",
    "            b[\"hlayer\" + str(i)] = np.random.uniform(-0.5,0.5,size=(hid_size))\n",
    "\n",
    "            \n",
    "    for i in range(max_iter):\n",
    "                \n",
    "        #GET INDEX OF RANDOM TRAINING SAMPLE\n",
    "        select = random.randint(0, len(attribute) - 1)\n",
    "\n",
    "        #GET THE TRAINING VALUES AND THE OUTPUT(Y) VALUE\n",
    "        x = attribute[select]\n",
    "        yval = label[select]\n",
    "        \n",
    "        \n",
    "        ######################\n",
    "        #FORWARD PROPOGATION\n",
    "        ######################\n",
    "        \n",
    "        hid_node_z = {}\n",
    "        hid_node = {}\n",
    "        \n",
    "        #FORWARD PROPAGATION FOR ALL THE HIDDEN LAYERS\n",
    "        for j in range(hid_layers):\n",
    "            if (j == 0):\n",
    "                hid_node_z[\"hlayer\" + str(j)] = np.dot(x,hidnew[\"hlayer\" + str(j)])\n",
    "                hid_node_z[\"hlayer\" + str(j)] = np.add(hid_node_z[\"hlayer\" + str(j)], b[\"hlayer\" + str(j)])\n",
    "                hid_node[\"hlayer\" + str(j)] = sig(hid_node_z[\"hlayer\" + str(j)])\n",
    "                \n",
    "            else:\n",
    "                hid_node_z[\"hlayer\" + str(j)] = np.dot(hid_node[\"hlayer\" + str(j-1)],hidnew[\"hlayer\" + str(j)])\n",
    "                hid_node_z[\"hlayer\" + str(j)] = np.add(hid_node_z[\"hlayer\" + str(j)], b[\"hlayer\" + str(j)])\n",
    "                hid_node[\"hlayer\" + str(j)] = sig(hid_node_z[\"hlayer\" + str(j)]) \n",
    "                \n",
    "            if (j == hid_layers-1):\n",
    "                \n",
    "                output = np.dot(hid_node[\"hlayer\" + str(j)], out_w) + b_out\n",
    "\n",
    "                #GET OUTPUT\n",
    "                y_hat = sig(output)\n",
    "                \n",
    "                j_prev = j_cur\n",
    "                j_cur = -(yval*np.log(y_hat)+(1-yval)*np.log(1-y_hat))\n",
    "                \n",
    "                cost_total += j_cur\n",
    "                \n",
    "                if(i%1675==0):\n",
    "                    \n",
    "                    epoch = epoch + 1\n",
    "                    print(\"Cost function for this spoch is \", (cost_total/1675))\n",
    "                    cost_total = 0\n",
    "                \n",
    "                #RETURN IF WE REACH THE NUMBER OF EPOCHS OR THE THRESHOLD\n",
    "                if(i!=0 and abs(j_prev - j_cur) <= thresh or epoch == 40):\n",
    "                    return hidnew, out_w, b, b_out, hid_size\n",
    "                \n",
    "                ##NOW WE NEED TO BACKPROPAGATE\n",
    "                delta_z_out = y_hat - yval\n",
    "                delta_w_out = delta_z_out * hid_node[\"hlayer\" + str(j)]\n",
    "                delta_b_out = delta_z_out\n",
    "                \n",
    "        deriv_activ = {}\n",
    "        delta_z_hid = {}\n",
    "        delta_z_hidden = {}\n",
    "        delta_w_hidden = {}\n",
    "        delta_b_hidden = {}\n",
    "                \n",
    "        for j in reversed(range(hid_layers)):\n",
    " \n",
    "            #IF WE ARE AT THE LAST HIDDEN LAYER\n",
    "            if (j == hid_layers-1):\n",
    "        \n",
    "                #NOW WE BACKPROPOGATE TO THE HIDDEN LAYER \n",
    "                deriv_activ[\"hlayer\" + str(j)] = [sig_deriv(x) for x in hid_node_z[\"hlayer\" + str(j)]]        \n",
    "                delta_z_hid[\"hlayer\" + str(j)] = np.multiply(delta_z_out, out_w)    \n",
    "                delta_z_hidden[\"hlayer\" + str(j)] = np.multiply(delta_z_hid[\"hlayer\" + str(j)], deriv_activ[\"hlayer\" + str(j)])\n",
    "\n",
    "                #GET DELTA W FOR EACH NODE OF THE HIDDEN LAYER\n",
    "                delta_w_hidden[\"hlayer\" + str(j)] = np.multiply(delta_z_hidden[\"hlayer\" + str(j)],hid_node[\"hlayer\" + str(j)])\n",
    "\n",
    "                #DELTA B IS EQUAL TO DELTA Z \n",
    "                #print(\"hlayer\" + str(i))\n",
    "                delta_b_hidden[\"hlayer\" + str(j)] = delta_z_hidden[\"hlayer\" + str(j)]\n",
    "            \n",
    "            #ALL OTHER HIDDEN LAYERS\n",
    "            else:\n",
    "                \n",
    "                #NOW WE BACKPROPOGATE TO THE HIDDEN LAYER \n",
    "                deriv_activ[\"hlayer\" + str(j)] = [sig_deriv(x) for x in hid_node_z[\"hlayer\" + str(j)]]   \n",
    "                delta_z_hid[\"hlayer\" + str(j)] = np.dot(delta_z_hidden[\"hlayer\" + str(j+1)], hidnew[\"hlayer\" + str(j+1)])    \n",
    "                delta_z_hidden[\"hlayer\" + str(j)] = np.multiply(delta_z_hid[\"hlayer\" + str(j)], deriv_activ[\"hlayer\" + str(j)])\n",
    "                \n",
    "                #GET DELTA W FOR EACH NODE OF THE HIDDEN LAYER\n",
    "                delta_w_hidden[\"hlayer\" + str(j)] = np.multiply(delta_z_hidden[\"hlayer\" + str(j)],hid_node[\"hlayer\" + str(j)])\n",
    "                \n",
    "                #DELTA B IS EQUAL TO DELTA Z \n",
    "                delta_b_hidden[\"hlayer\" + str(j)] = delta_z_hidden[\"hlayer\" + str(j)]        \n",
    "        \n",
    "        \n",
    "    \n",
    "        for j in reversed(range(hid_layers)):\n",
    "            \n",
    "            val = np.multiply(learn_rate,delta_b_hidden[\"hlayer\" + str(j)])\n",
    "            b[\"hlayer\" + str(j)] = np.subtract(b[\"hlayer\" + str(j)], val)\n",
    "\n",
    "            \n",
    "            ###THE PROBLEM IS WITH DELTA W HIDDEN\n",
    "            valnew = np.multiply(learn_rate,delta_w_hidden[\"hlayer\" + str(j)])\n",
    "            \n",
    "            \n",
    "            hidnew[\"hlayer\" + str(j)] = np.subtract(hidnew[\"hlayer\" + str(j)], valnew)\n",
    "        \n",
    "        \n",
    "        val = np.multiply(learn_rate,delta_w_out)\n",
    "        out_w = np.subtract(out_w, val)\n",
    "        \n",
    "        val = np.multiply(learn_rate,delta_b_out)\n",
    "        b_out = np.subtract(b_out, val)\n",
    "    \n",
    "\n",
    "    \n",
    "    return hidnew, out_w, b, b_out, hid_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MUST ODIFY TO WORK WITH THE NEW EXRTA LAYER\n",
    "def predict2(hidden_weights, output_weights, bs_1, bs_2, attributes, labels, hid_layers):\n",
    "    correct = 0\n",
    "    total = 0    \n",
    "    \n",
    "    #LOOP THROUGH ALL THE TEST CASES \n",
    "    for t in range(len(attributes)):        \n",
    "        \n",
    "        hid_node_z_new = {}\n",
    "        hid_node_new = {}\n",
    "        \n",
    "        for i in range(hid_layers):\n",
    "            \n",
    "            #FORWARD PROPAGATION\n",
    "            #HIDDEN LAYER\n",
    "            if (i == 0):\n",
    "                nval = np.dot(attributes[t],hidden_weights[\"hlayer\" + str(i)])\n",
    "                hid_node_z_new[\"hlayer\" + str(i)] = np.add(nval, bs_1[\"hlayer\" + str(i)])\n",
    "                hid_node_new[\"hlayer\" + str(i)] = sig(hid_node_z_new[\"hlayer\" + str(i)]) \n",
    "\n",
    "            else:\n",
    "                nval = np.dot(hid_node_new[\"hlayer\" + str(i-1)],hidden_weights[\"hlayer\" + str(i)])\n",
    "                hid_node_z_new[\"hlayer\" + str(i)] = np.add(nval, bs_1[\"hlayer\" + str(i)])\n",
    "                hid_node_new[\"hlayer\" + str(i)] = sig(hid_node_z_new[\"hlayer\" + str(i)])     \n",
    "                \n",
    "        hid_node_new_val = hid_node_new[\"hlayer\" + str(i)]\n",
    "            \n",
    "        #OUTPUT LAYER\n",
    "        output = np.dot(hid_node_new_val, output_weights) + bs_2 \n",
    "        y_hat = sig(output)\n",
    "\n",
    "        \n",
    "        #IF THE YHAT VALUE IS ABOVE 0.5 SET IT TO 1 AND IF LESS SET TO 0\n",
    "        if y_hat >= 0.5:\n",
    "            y_hat = 1\n",
    "        else:\n",
    "            y_hat = 0\n",
    "            \n",
    "            \n",
    "        #IF THE PREDICTION IS CORRECT INCREASE CORRECT \n",
    "        if (y_hat == labels[t]):\n",
    "            correct = correct + 1\n",
    "            total = total + 1\n",
    "        else:\n",
    "            total = total + 1\n",
    "        \n",
    "    \n",
    "    #RETURN ACCURACY\n",
    "    accuracy = (correct/total)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost function for this spoch is  [2.01766451e-05]\n",
      "Cost function for this spoch is  [1.31667832]\n",
      "Cost function for this spoch is  [1.00090121]\n",
      "Cost function for this spoch is  [0.96105789]\n",
      "Cost function for this spoch is  [0.91457859]\n",
      "Cost function for this spoch is  [0.87195607]\n",
      "Cost function for this spoch is  [0.81610489]\n",
      "Cost function for this spoch is  [0.78939338]\n",
      "Cost function for this spoch is  [0.7793564]\n",
      "Cost function for this spoch is  [0.70327609]\n",
      "Cost function for this spoch is  [0.70587692]\n",
      "Cost function for this spoch is  [0.64111296]\n",
      "Cost function for this spoch is  [0.64840017]\n",
      "Cost function for this spoch is  [0.61516547]\n",
      "Cost function for this spoch is  [0.64007675]\n",
      "Cost function for this spoch is  [0.58890889]\n",
      "Cost function for this spoch is  [0.56825849]\n",
      "Cost function for this spoch is  [0.56727532]\n",
      "Cost function for this spoch is  [0.55874823]\n",
      "Cost function for this spoch is  [0.5574643]\n",
      "Cost function for this spoch is  [0.53626105]\n",
      "Cost function for this spoch is  [0.57779247]\n",
      "Cost function for this spoch is  [0.50935783]\n",
      "Cost function for this spoch is  [0.54002819]\n",
      "Cost function for this spoch is  [0.47610361]\n",
      "Cost function for this spoch is  [0.47610896]\n",
      "Cost function for this spoch is  [0.51473432]\n",
      "Cost function for this spoch is  [0.49139593]\n",
      "Cost function for this spoch is  [0.4838541]\n",
      "Cost function for this spoch is  [0.45346556]\n",
      "Cost function for this spoch is  [0.47223898]\n",
      "Cost function for this spoch is  [0.45519249]\n",
      "Cost function for this spoch is  [0.46787252]\n",
      "Cost function for this spoch is  [0.4610872]\n",
      "Cost function for this spoch is  [0.47563947]\n",
      "training done\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "max_iter = 16750000\n",
    "learn_rate = 0.001\n",
    "thresh = 0.000001\n",
    "hid_layers = 2\n",
    "\n",
    "num_attr, train_attributes, test_attributes, train_labels, test_labels= load_cifar_data()\n",
    "\n",
    "hidnew, out_w, b, b_out, hid_size = training2(num_attr, max_iter, train_attributes, train_labels, learn_rate, thresh, hid_layers)\n",
    "print(\"training done\")\n",
    "accuracy = predict2(hidnew, out_w, b, b_out, test_attributes, test_labels, hid_layers)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results shown here are the results of running the algorithm with two hidden layers. This result slightly better in accuracy than the algorithm with just one hidden layer. It achieves an accuracy of 3% better. This improvement is not seen once more layers are added however. The more hidden layers that are added the further the performance of the algorithm degrades. I believe this may be due to the vanishing gradient problem that becomes more of a problem as more layers are added. This issue means that as the derivates are multiplied by each other when backproagating meaning the gradient will be smaller. This means that in the layers at the beginning of the network will experience very little change when training. This is a common issue when using the sigmoid activation function and therefore a better implementation would be to use the tanh function or another such activation function. This is subsequent work that could be carried out. We can see this decrease in performance in the use of 3 layers below as the performance decreases to 65% which is about a 10% decrease in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost function for this spoch is  [5.165084e-05]\n",
      "Cost function for this spoch is  [1.40804942]\n",
      "Cost function for this spoch is  [1.06017571]\n",
      "Cost function for this spoch is  [0.92657833]\n",
      "Cost function for this spoch is  [0.86878052]\n",
      "Cost function for this spoch is  [0.81086256]\n",
      "Cost function for this spoch is  [0.79908394]\n",
      "training done\n",
      "0.65\n"
     ]
    }
   ],
   "source": [
    "max_iter = 16750000\n",
    "learn_rate = 0.001\n",
    "thresh = 0.000001\n",
    "hid_layers = 3\n",
    "\n",
    "num_attr, train_attributes, test_attributes, train_labels, test_labels= load_cifar_data()\n",
    "\n",
    "hidnew, out_w, b, b_out, hid_size = training2(num_attr, max_iter, train_attributes, train_labels, learn_rate, thresh, hid_layers)\n",
    "print(\"training done\")\n",
    "accuracy = predict2(hidnew, out_w, b, b_out, test_attributes, test_labels, hid_layers)\n",
    "\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
